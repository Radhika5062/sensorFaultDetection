{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Github Repo\n",
    "# Step 2: Create a local folder\n",
    "# Step 3: Create Virtual Environment\n",
    "# Step 4: Connect our Vs code with Github repo using git clone in terminal\n",
    "# Step 5: Create a folder called Setup. Now add init file to create it as a package. \n",
    "# It is important to convert this folder to package so that we can import the components of this package into other places. \n",
    "# Step 6: Create setup.py file. \n",
    "# Setup.py is created for many purposes:\n",
    "    # to install components\n",
    "    # manage dependencies\n",
    "    # manage distributions\n",
    "    # all the dependencies, packages etc that we need to install, we do it via this file. \n",
    "# step 7: Run the setup file via terminal using python setup.py install command. If you will not use install then it will\n",
    "# throw error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Now what is the purpose of egg-info file. So basically whenever we create a package using setup.py then we need\n",
    "# to store the information about the package like its dependencies, metadata etc. so for every package this egg-info file\n",
    "# gets created which stores this information for the package. \n",
    "# When you see the egg-info file then you can surely say that a folder is a package and its contents can be used outside \n",
    "# by importing it. \n",
    "# Step 9: Now we need to create requirements.txt file.\n",
    "# Step 10: Add the install_requires line in the setup.py file. This tells setup.py file that it needs to install\n",
    "# the said dependencies or libraries. \n",
    "# Step 11: run the requirements.txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Day 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Add the get_requirements() function in the setup.py file and remove the list against the install_packages\n",
    "# line in setup.py.\n",
    "# Add -e . in the requirements.txt file. This is called editable mode. Usually when we run the requirements.txt file then \n",
    "# after this we need to run the setup.py file to find out the packages as packages = find_packages() written in the setup.py\n",
    "# file helps in finding all the packages in our root directory. Now when we write -e . in the requirements.txt file then \n",
    "# what it does is - it automatically runs the setup,.py file which will then start finding the packages. \n",
    "# If we do not have -e . in requirements.txt then we will need to run setup.py manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Define a function called get_requirements which helps in fetching the requirements and then call this function\n",
    "# in the install_packages instead of adding the package names one by one. \n",
    "# Step 14: Create the exception.py file and logging file. \n",
    "# In exception: We have to deal with error message and error message detail. Inherit the exception class. \n",
    "# In logging: We need the filename, level and format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset:\n",
    "# https://raw.githubusercontent.com/avnyadav/sensor-fault-detection/main/aps_failure_training_set1.csv\n",
    "# EDA.ipynb file\n",
    "# EDA Steps\n",
    "# 1. Reading the data in python\n",
    "# 2. Defining the problem statement\n",
    "# 3. Identifying the target variable\n",
    "# 4. Looking at the distribution of the target variable\n",
    "# 5. Basic data exploration\n",
    "# 6. Rejecting useless columms\n",
    "# 7. Visual explanatory data analysis for data distribution (histogram and bar charts)\n",
    "# 8. Feature selection based on data distribution\n",
    "# 9. Outlier treatment\n",
    "# 10. Missing values treatment\n",
    "# 11. Statistical correlation analysis(Feature Selection)\n",
    "# 12. Converting data to numeric for ML\n",
    "# 13. Sampling and k fold cross validation\n",
    "# 14. Trying multiple regression algorithms\n",
    "# 15. Selecting the best model \n",
    "# 16. Deploying the best model in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: connect with mongo db. \n",
    "# Create cluster -> Database user -> Network access -> connect with driver and copy the driver url.\n",
    "# mongodb+srv://Radhika:<password>@cluster0.vgtvmch.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 16: Go to mongodb.ipynb file. This is a sample file to show how to connect to mongodb atlas to fetch and retrieve data\n",
    "# from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mongodb atlas is cloud application. Mongodb Compass is a desktop application. Mongodb compass gives us Graphical User\n",
    "# Interface to make transformations on data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17: Create the requirements.txt file. Ensure to have the python-dotenv in it along with -e . \n",
    "# Step 18: Create .env file in the root directory and add the moongo db url there.\n",
    "# step 19: Go to init file in the sensor directory and write the code. \n",
    "# Reading the .env file.\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# step 20: Now we need to write the code to send the data file that we have on to mongo db . For this create 2 files called utlils.py\n",
    "# and config.py in the sensor folder. \n",
    "# Check the code in config.py file under the sensor folder. Here we are fetching the string Mongo db connection url which was\n",
    "# defined in the .env folder and then making the connection by creating a client. \n",
    "\n",
    "# from dataclasses import dataclass\n",
    "# import os\n",
    "# import pymongo \n",
    "\n",
    "# @dataclass\n",
    "\n",
    "# class EnvironmentVariable:\n",
    "#     mongo_db_url:str = os.getenv(\"MONGO_DB_URL\") # This is a constant which we have defined in the .env file. \n",
    "\n",
    "# # Now we create an object of this class\n",
    "# env_var = EnvironmentVariable()\n",
    "\n",
    "# # Now we need to create the mongo client\n",
    "# # env_var.mongo_db_url is the way we can access the class variable. object.variable name\n",
    "# mongo_client = pymongo.MongoClient(env_var.mongo_db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 21: Write the utils.py file\n",
    "\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# from sensor.config import mongo_client\n",
    "\n",
    "# def dump_csv_file_to_mongodb_collection(\n",
    "#         file_path:str, \n",
    "#         database_name:str, \n",
    "#         collection_name:str) -> None:\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         df.reset_index(drop=True, inplace=True)\n",
    "#         json_records = list(json.loads(df.T.to_json()).values())\n",
    "#         mongo_client[database_name][collection_name].insert_many(json_records)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "\n",
    "# Step 22: Go to main and add the code to call the dump_csv_file_to_mongodb_collection. give the file path, database name\n",
    "# and the collection name.\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     file_path = \"C:/Users/RadhikaMaheshwari/Desktop/Test/DeepLearning/ETE2/aps_failure_training_set1.csv\"\n",
    "#     database_name = \"ineuron\"\n",
    "#     collection_name = 'sensor'\n",
    "#     dump_csv_file_to_mongodb_collection(file_path, database_name, collection_name)\n",
    "\n",
    "\n",
    "# step 23: run the main.py file. the database will get created in mongodb. Double check by logging into mongo db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 24: Creating folders. We will create a few folders now and some later when required.\n",
    "# First create a foler under root folder called \"config\" and then create a file called \"schema.yaml\" \n",
    "# Now under sensor create folders as follows. Do create __init__ file under each folder. \n",
    "# 1. cloud_storage - __init__.py\n",
    "# 2. components - __init__.py\n",
    "    # data_ingestion.py\n",
    "    # data_validation.py\n",
    "    # data_transformation.py\n",
    "    # model_trainer.py\n",
    "    # model_evaluation.py\n",
    "    # model_pusher.py\n",
    "# 3. configuration\n",
    "    # __init__.py\n",
    "    # mongodb_db_connection.py\n",
    "# 4. constant\n",
    "    # __init__.py\n",
    "    # env_variable.py\n",
    "    # application.py\n",
    "    # database.py\n",
    "    # s3_bucket.py\n",
    "    # Folder - training_pipeline : init\n",
    "# 5. data_access\n",
    "    # init \n",
    "# 6. entity\n",
    "    # init \n",
    "    # artifact_entity.py\n",
    "    # config_entity.py\n",
    "# 7. pipeline\n",
    "    # init \n",
    "    # training_pipeline.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25: Create the schema.yaml file. \n",
    "# step 26: Fill up the init file in constant> training_pipeline folder till data ingestion config details. \n",
    "# Step 27: Fill up the config_entity file. Have added notes there on what it is about. It is simply for creating the\n",
    "# file and folder structure that is defined in the init file of constant > training pipeline folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 28: New way after explaning how the different components interact with each other. \n",
    "# Create constant > training_pipeline > init. Here all the constants will get stored. \n",
    "# Create entity > artifact_entity.py. Here we are just preparing the artifact of data ingestion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step A : Create constant > training_pipeline > init. Here we define all the constants.\n",
    "# step B : Create entity>config.py file. Here we define training pipeline class along with dataingestion class. The main purpose \n",
    "# of this is to create methods to create the directory paths. Also create entity>aritifact file where we will define the \n",
    "# testing and training dataset paths. \n",
    "# step C : Now create .env file. In this we define our mongo db actual url.\n",
    "# step D : Create the constant > env_variable file. In this we will just give mongo db constants.\n",
    "# Step E : Create constant > database.py file. In this we will define our database name and the collection name.\n",
    "# Step F : Create configuration>mongodb connection file. In this we are just building the connection to mongo db. \n",
    "# Step G : Create constant > data_access> sensor_data file. In constructor you need to use the mongo db connection created in\n",
    "# the file in step F. Also we define two functions which will send and receive data from mongo db. \n",
    "# Step H: Create component > data_ingestion file. In the constructor we will call the data ingestion config. Create another function\n",
    "# for 4 things - First function is for creating sensor.csv file in the feature_store folder. So in this one we will first get the \n",
    "# data from mongodb > then we will get the path from entity>config file and then we will create this directory and then will\n",
    "# store the data in the form of a dataframe in this csv file. Now the second function that we will be creating is the train test split\n",
    "# where we will be splitting the data into training set and testing set and then we will store them in individual files. \n",
    "# Last function in this file will be just tying up the above functions. So we will just be calling everything just like treating this\n",
    "# as main function\n",
    "#this is how data ingestion is now completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion is complete. Data validation starts now. \n",
    "# Step A: Create the config>schema.yaml file. We just need to provide all the column names along with their data types, \n",
    "# numerical column names and also a list of column names to drop.\n",
    "# Now we will need to be able to read this schema.yaml file which will require creating a function which will read this file. \n",
    "# Step B: For this, we will create a folder called utils in sensor. Then we will create __init__ file so that this folder can be \n",
    "# used as a package. then we will create a file called main_utils inside this. \n",
    "# Step C: In requirements.txt file add two things - pyYAML and dill.\n",
    "# Step D: Now in main_utils.py file, create def read_yaml. \n",
    "# Step E: Now in data validation, we will also need to create report.yaml file. So we will need to create a function which will \n",
    "# help us in creating this file. We will create this function in the main_utils.py file. \n",
    "# Step F: Now we will start creating what we have in our flow chart. \n",
    "# Step G: Now first thing is, we need to add constants. This will be added in constant>training_pipeline>init file. \n",
    "# Step H: We need to create the entity>artifact file. Artifact is the output that will come from the data validation stage. So, go\n",
    "# into entity>artifact and create this file. In this, we just have to define what will be output and what will be its datatype. \n",
    "# Step I: Now we need to get these constants and create their paths. This will be done in entity>config file. \n",
    "# Step J: Now we will be creating data validation component. In this we will be doing 5 tasks -\n",
    "# Task 1: Read the test and trained datasets\n",
    "# Task 2: Validate the presence of required columns in both the datasets.\n",
    "# Task 3: Validate the presence of required numerical columns in both the datasets.\n",
    "# Task 4: Detect data drift between the trained and test datasets. \n",
    "# Task 5: Construct and return a data validation artifact object containing validation results and file paths.\n",
    "# There will be three inputs to this component - Ingestion artifact, validation config, schema file. For creating this file, go into \n",
    "# components > data validation file.\n",
    "# Step K: Now it is time to create pipeline>training_pipeline for data validation. This will need to be done for data ingestion too.\n",
    "# We forgot doing that part earlier so we have added everything here. \n",
    "# Now the data validation part is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation starts from here. \n",
    "# Step A: Add dill, scipy and imblearn in the requirements.txt file and run it in terminal.\n",
    "# Step B: Create folder called \"ml\" in sensor. then add init file in it. Then create another folder called \"model\" and then again \n",
    "# an init file in it. Create another file in model called \"estimator.py\". Model mapping will be kept here. \n",
    "# In sensor > utils > main_utils, we need to add function called save_numpy_array_data, this function will save the numpy array to \n",
    "# the file path location. \n",
    "# Step C: Create another function in sensor>utils>main_utils called load_numpy_array_data. This function will help in loading the\n",
    "# numpy array data whereever it is needed.\n",
    "# Step D: Now create another function in sensor>utils>main_utils called save_object\n",
    "# Step E: Now in sensor>ml>model>esitmator file, let us add our mapping of target variable and its reverse as well.\n",
    "# Step F: Now go to constant>training_pipeline>init and add the data transformation constants and define the following - \n",
    "# data transformation folder, transformed data directory name and transformed object directory name.\n",
    "# Step G: Now go into sensor>entity>artifact and add the data transformation artifacts. \n",
    "# Step H: Now add data transformation config in the entity>config file. \n",
    "# Step I: Now we will go into components and create the component>data_transformation \n",
    "# Step J: Add data transformation in training pipeline. \n",
    "# Data transformation is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting model training. \n",
    "# Step A: Add  xgboost==1.6.2 in requirements.txt file and run it in terminal.\n",
    "# Step B: Create load_object function in the main_utils.py file\n",
    "# Step C: Create another directory in ml called metric and add init file as well as classification_metrics.py. In this file we write \n",
    "# the concept that is used to check accuracy. Create classification metric artifact in artifact.\n",
    "# Step D: Crate a class called SensorModel in esitmator.py file.\n",
    "# Step E: Now add constants in constants>training pipeline>init file. \n",
    "# Step F: Now go into config file and add model trainer config\n",
    "# Step G: Create model trainer artifact in artifact.\n",
    "# Step H: Now create the model trainer component in components folder. \n",
    "# Step I: Create model trainer pipeline. \n",
    "# Model training complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast API\n",
    "# Step A: Create a file called fast.py\n",
    "# Step B: pip install fastapi in terminal\n",
    "# Step C: pip install uvicorn. Uvicorn is the server which will help us in running fastapi.\n",
    "# Now add the following code in fast api just to check the functionality of how it works: \n",
    "# from fastapi import FastAPI\n",
    "\n",
    "# app = FastAPI()\n",
    "\n",
    "# @app.get(\"/hello\")_\n",
    "# async def root():\n",
    "#     return \"hello world\"\n",
    "# Now to run this we will need to write the following in terminal: uvicorn fast:app reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation starts here\n",
    "# Step A: First we will add constants. also define the constant for saved_model_directory\n",
    "# Step B: Going into config file to create paths for the items which we added in constants\n",
    "# Step C: Create artifact for model evaluation\n",
    "# Step D: Create component for model evaluation\n",
    "# Step E : Create pipeline\n",
    "# Model Evaluation completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model pusher\n",
    "# In this, we are sending our best model to store for short term and long term use. \n",
    "# Step A: We will first add constants.\n",
    "# Step B: Add the config in entity.\n",
    "# Step C: Create artifact\n",
    "# Step D: Create component\n",
    "# Step E: Now add the training pipeline\n",
    "# Model pusher is now complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application\n",
    "# Till now we were running pipeline and we were getting answer on terminal. Now instead of using terminal, we will be showing\n",
    "# our results on Fast api. To do this we will have to perform the following:\n",
    "# 1. connect with fast api.\n",
    "# 2. Training model\n",
    "# 3. Model prediction\n",
    "# This will all be done in main.py\n",
    "# Important items:\n",
    "# 1. We should know Port.\n",
    "# 2. We should know host\n",
    "# 3. We will be creating paths for get and paths. And then decide what kind of functions - synchoronous or asynchronous\n",
    "#     functions should be used. \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step A: Create a file called application.py in constant>training pipeline if not done already. Define host and port in this\n",
    "# file. \n",
    "# Step B: Now go to main file and code fastapi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file outside called - \"deployment steps.txt\". In this we will write all the steps for deployment.\n",
    "# Create a folder - .github.\n",
    "# In that create another folder called workflows.\n",
    "# In workflows, create main.yml. This is not a yaml file. This file describes roadmap of deployment. \n",
    "# Now go into sensor > cloud storage > s3_syncer.py file\n",
    "# Now go into constant>s3 bucket file and add s3 related constanst\n",
    "# Update the env_variable file in constans with aws related constants\n",
    "# Now some changes need to be made in training pipleline in pipeline.\n",
    "# Create a file called Dockerfile\n",
    "# Create another file called .dockerignore. This is similar to gitignore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
